# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**
The dataset contains data about features of customers who either subscribed or didn't subscribe to a fixed term deposit with a financial institution. The goal is to predict whether or not a customer will subscribe given a bunch of features, e.g. job, education. 
**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**
The best performing model was the VotingEnsemble model from automl with an accuracy of 0.91590288, which uses multiple estimators to predict a class. This is to be expected, since logistic regression is one of the simplest models and an ensemble model, by definition, is a an aggregate of models. For the best performing hyperparameters found for logistic regression, the accuracy was 0.91001518 with a regularization parameter value (C) of 17.11553892 

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
Under a single workspace a cluster is provisioned. The actual hyperparameter tuning job creates containers for every hyperparameter it samples using the specified image in the configuration to run the train.py script. The train.py script then downloads the data from an external source, splits it into a training and test set and uses the hyperparameters passed to it from the container to log the obtained metric which coincides with the metric from the config, using an sklearn learn logistic regression model. The containers are essentially producers, in the streaming sense, that have their messages consumed by the hyperparameter job running to determine the best run and to apply the early termination policies specified in the configuration. 
**What are the benefits of the parameter sampler you chose?**
It is much faster than grid search/bayesian/evolutionary without sacrificing much (if any) efficacy. It also gives flexibility for swapping out sample distributions to incorporate more knowledge about the parameter search space (in this case, it was chosen to be uniform, having maximum entropy implying that nothing is assumed/known about the parameter space distribution).   
**What are the benefits of the early stopping policy you chose?**
BanditPolicy stops after it stops seeing sufficient improvements from the best found metric after a specified number of iteratons for every interval following of a specificed size. This gives a burn-in period for a sufficient number of iterations before the policy is applied and a sufficient interval delay, so that a premature termination can be avoided (through sampling variance control by the interval parameter). 

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
 VotingEnsemble with an accuracy of	0.91590288.	Since the ensemble consists of 8 models, each with its own hyperparameters, I am excluding them here, since there are many. 
## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
The accuracy difference is 0.91590288 (automl) vs. 0.91001518 (hyperparameter tuned logistic regression). The accuracy differences are not that significant, however, this might be due to the lack of class balancing for the hyperparameter turned job, i.e. it might be predicting the frequent class too often. The architectural differences are superficial, other than the usage of the local VM instead of the provisioned cpu cluster. If the cpu cluster were to be used, a dataset would be necessary, which is, analogous to its spark equivalent, an external datasource in the workspace that can be used as such (either as a spark/pandas dataframe, lazily evaluated when used by automl). Each job for the automl is a model with selected hyperparameters, while each job for the hyperparameter tuning job is the same model with a new set of hyperparameters chosen in the specified parameter space. An obvious difference is that no container needs to be specified for automl, while a container (either prebuilt, or otherwise) needs to be made available for the hyperparameter tuning job. 

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
As indicated by the automl run, there is severe imbalance in the sample data, i.e. there are hardly any customers who subscribed compared to those who did not. So applying SMOTE to generate more samples for y == 1 might be beneficial. 

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
